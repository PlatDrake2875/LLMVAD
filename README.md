# LLMVAD: Large Language Model-based Video Anomaly Detection

This repository, `PlatDrake2875-LLMVAD`, is a **Large Language Model-based Video Anomaly Detection** system. It processes video files by extracting frames, analyzing their content using HuggingFace's Gemma 3 multimodal model, and then applying an anomaly detection model to these summaries to identify unusual events. The system can then plot and save the anomaly scores.

## Features

* **Video Frame Extraction:** Extracts frames from various video formats at a configurable interval.

* **Multimodal LLM-based Analysis:** Utilizes HuggingFace's Gemma 3 multimodal model to generate descriptive summaries directly from video frames and text descriptions.

* **Anomaly Detection:** Employs a local Gemma 3-based anomaly detector to score video segments for unusual activity.

* **Anomaly Score Visualization:** Generates and saves plots of anomaly scores over time for easy interpretation.

* **Persistent Data Storage:** Saves frame descriptions and anomaly scores as pickled files for later analysis.

## Project Structure

The project is organized into several Python modules:

* `main.py`: The entry point for the application. It handles argument parsing, initializes the core components, and orchestrates the video processing and anomaly detection workflows.

* `video_processor.py`: Manages the reading of video files, extraction of frames, and interaction with the `HuggingFaceGemmaClient` for frame summarization. It also handles pickling the generated descriptions.

* `gemma_client.py`: Provides a client to interact with HuggingFace's Gemma 3 multimodal model. It includes methods for image-to-text generation, text generation, summarizing individual frames, and summarizing chunks of video descriptions.

* `anomaly_detector.py`: Contains the `AnomalyDetector` class, which uses the Gemma 3 model to judge video summaries for anomalies based on a configured prompt and system role. It extracts numerical anomaly scores from the model's responses.

* `plotting.py`: Provides the `plot_anomaly_scores` function to visualize the anomaly scores generated by the `AnomalyDetector`, saving them as PNG images.

* `utils.py`: Contains utility functions such as `setup_logging` for application-wide logging configuration and `_get_summaries` for loading pickled summary files.

* `config.py`: Defines default configuration parameters for the video directory, frame interval, Gemma 3 model name, and device settings.

* `requirements.txt`: Lists all Python dependencies required for the project.

* `.gitignore`: Specifies files and directories to be ignored by Git (e.g., Python bytecode, virtual environments, datasets).

## Setup

1. **Clone the repository:**

```bash
git clone https://github.com/PlatDrake2875/LLMVAD.git
cd LLMVAD
```

2. **Set up a Python virtual environment (recommended):**

```bash
python -m venv venv
source venv/bin/activate  # On Windows, use venv\Scripts\activate
```

3. **Install dependencies:**

```bash
pip install -r requirements.txt
```

The project requires PyTorch, Transformers (version 4.50.0+), and other dependencies for running HuggingFace's Gemma 3 multimodal model locally.

4. **Configure HuggingFace Authentication:**

Create a `.env` file in the project root and add your HuggingFace API key:

```bash
# .env file
HUGGINGFACE_API_KEY=your_actual_hf_api_key_here
```

To get your HuggingFace API key:
1. Go to [HuggingFace Settings > Access Tokens](https://huggingface.co/settings/tokens)
2. Create a new token with "Read" permissions
3. Copy the token and paste it in your `.env` file

**Note:** The `.env` file is automatically ignored by git to keep your API key secure.

## Hardware Requirements

* **GPU (Recommended):** A CUDA-compatible GPU with at least 8GB VRAM for optimal performance with the Gemma 3 4B model.
* **CPU:** The model can run on CPU but will be significantly slower.
* **RAM:** At least 16GB system RAM recommended for the 4B model.

## Usage

The `main.py` script supports two primary modes:

1. **Video Processing and Summarization (`main()` function):** Extracts frames, analyzes them with Gemma 3's vision capabilities, and pickles the individual frame descriptions and chunked summaries.

2. **Anomaly Detection (`anomaly_detection()` function):** Loads the pickled summaries, performs anomaly detection, plots the scores, and saves the anomaly scores locally.

By default, the `main.py` script is set to run `anomaly_detection(args_anomaly)`. You can uncomment `main()` to run video processing first.

### Running Video Processing and Summarization

To process videos and generate summaries:

```bash
python main.py --video_dir "path/to/your/videos" --K 10 --summarization_chunk_size 3 --gemma_model "google/gemma-3-4b-it" --device "auto"
```

* `--video_dir`: (Optional) Directory containing your video files. Defaults to `datasets/XD_Violence_1-1004`.
* `-K`: (Optional) Process every Kth frame. Defaults to 10.
* `--summarization_chunk_size`: (Optional) Number of frame descriptions to group into one chunk for summarization. Defaults to 3.
* `--gemma_model`: (Optional) Name of the HuggingFace Gemma 3 model to use. Defaults to `google/gemma-3-4b-it`.
* `--device`: (Optional) Device to run the model on ('auto', 'cuda', 'cpu'). Defaults to 'auto'.

This will generate `*_frame_descriptions.pkl` and `*_chunked_summaries.pkl` files in your video directory.

### Running Anomaly Detection

To perform anomaly detection on pre-processed summaries and plot the results:

```bash
python main.py --video_dir "path/to/your/videos" --gemma_model "google/gemma-3-4b-it" --device "auto"
```

* `--video_dir`: (Optional) Directory containing the video files and their pickled summaries. Defaults to `datasets/XD_Violence_1-1004`.
* `--gemma_model`: (Optional) Name of the HuggingFace Gemma 3 model to use. Defaults to `google/gemma-3-4b-it`.
* `--device`: (Optional) Device to run the model on ('auto', 'cuda', 'cpu'). Defaults to 'auto'.

This will load the chunked summaries, run the anomaly detection, print the model's judgments, save `*_anomaly_scores.pkl`, and generate an `*_anomaly_scores.png` plot in the video directory.

**Note:** Ensure you have videos in the specified `video_dir` and that `process_video` has been run at least once to generate the summary `.pkl` files before running `anomaly_detection`.

## Key Advantages of Gemma 3

* **Multimodal Capabilities:** Unlike text-only models, Gemma 3 can directly analyze images, providing more accurate and detailed video frame descriptions.
* **Large Context Window:** 128K tokens context allows for processing longer video sequences.
* **No External Dependencies:** Runs completely locally without requiring external API services.
* **High Performance:** State-of-the-art performance on vision-language tasks.





