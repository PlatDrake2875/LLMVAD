# LLMVAD: Large Language Model-based Video Anomaly Detection

This repository, `PlatDrake2875-LLMVAD`, is a **Large Language Model-based Video Anomaly Detection** system. It processes video files by extracting frames, summarizing their content using an Ollama-compatible LLM, and then applying an anomaly detection model to these summaries to identify unusual events. The system can then plot and save the anomaly scores.

## Features

* **Video Frame Extraction:** Extracts frames from various video formats at a configurable interval.

* **LLM-based Summarization:** Utilizes Ollama to generate descriptive summaries of individual video frames and chunks of frames.

* **Anomaly Detection:** Employs a rolling-context LLM-based anomaly detector to score video segments for unusual activity.

* **Anomaly Score Visualization:** Generates and saves plots of anomaly scores over time for easy interpretation.

* **Persistent Data Storage:** Saves frame descriptions and anomaly scores as pickled files for later analysis.

## Project Structure

The project is organized into several Python modules:

* `main.py`: The entry point for the application. It handles argument parsing, initializes the core components, and orchestrates the video processing and anomaly detection workflows.

* `video_processor.py`: Manages the reading of video files, extraction of frames, and interaction with the `OllamaClient` for frame summarization. It also handles pickling the generated descriptions.

* `ollama_client.py`: Provides a client to interact with the Ollama API. It includes methods for converting images to base64, summarizing individual frames, and summarizing chunks of video descriptions.

* `anomaly_detector.py`: Contains the `AnomalyDetector` class, which uses the Ollama API to judge video summaries for anomalies based on a configured prompt and system role. It extracts numerical anomaly scores from the LLM's responses.

* `plotting.py`: Provides the `plot_anomaly_scores` function to visualize the anomaly scores generated by the `AnomalyDetector`, saving them as PNG images.

* `utils.py`: Contains utility functions such as `setup_logging` for application-wide logging configuration and `_get_summaries` for loading pickled summary files.

* `config.py`: Defines default configuration parameters for the video directory, frame interval, Ollama URL, model name, and API timeout.

* `.gitignore`: Specifies files and directories to be ignored by Git (e.g., Python bytecode, virtual environments, datasets).

## Setup

1. **Clone the repository:**

git clone https://github.com/PlatDrake2875/LLMVAD.git
cd LLMVAD


2. **Set up a Python virtual environment (recommended):**

python -m venv venv
source venv/bin/activate  # On Windows, use venv\Scripts\activate


3. **Install dependencies:**
The project relies on libraries such as `opencv-python`, `Pillow`, `requests`, `matplotlib`, and `argparse`. You'll need to install them. (A `requirements.txt` file is not provided, so these are inferred.)

pip install opencv-python Pillow requests matplotlib


4. **Install and run Ollama:**
This project requires an Ollama server running locally to provide the LLM capabilities.

* Download and install Ollama from [ollama.com](https://ollama.com/).

* Pull the required model (default is `gemma3:4b-it-q4_K_M`).

  ```
  ollama run gemma3:4b-it-q4_K_M
  
  ```

  Ensure the Ollama server is running and accessible at the URL specified in `config.py` (default: `http://localhost:11434/api/chat`).

## Usage

The `main.py` script supports two primary modes:

1. **Video Processing and Summarization (`main()` function):** Extracts frames, summarizes them, and pickles the individual frame descriptions and chunked summaries.

2. **Anomaly Detection (`anomaly_detection()` function):** Loads the pickled summaries, performs anomaly detection, plots the scores, and saves the anomaly scores locally.

By default, the `main.py` script is set to run `anomaly_detection(args_anomaly)`. You can uncomment `main()` to run video processing first.

### Running Video Processing and Summarization

To process videos and generate summaries:

python main.py --video_dir "path/to/your/videos" --K 10 --summarization_chunk_size 3 --ollama_url "http://localhost:11434/api/chat" --ollama_model "gemma3:4b-it-q4_K_M" --timeout 30


* `--video_dir`: (Optional) Directory containing your video files. Defaults to `datasets/XD_Violence_1-1004`.

* `-K`: (Optional) Process every Kth frame. Defaults to 10.

* `--summarization_chunk_size`: (Optional) Number of frame descriptions to group into one chunk for summarization. Defaults to 3.

* `--ollama_url`: (Optional) URL for the Ollama chat API endpoint. Defaults to `http://localhost:11434/api/chat`.

* `--ollama_model`: (Optional) Name of the Ollama model to use. Defaults to `gemma3:4b-it-q4_K_M`.

* `--timeout`: (Optional) Timeout for Ollama API requests in seconds. Defaults to 30.

This will generate `*_frame_descriptions.pkl` and `*_chunked_summaries.pkl` files in your video directory.

### Running Anomaly Detection

To perform anomaly detection on pre-processed summaries and plot the results:

python main.py --video_dir "path/to/your/videos" --ollama_url "http://localhost:11434/api/chat" --ollama_model "gemma3:4b-it-q4_K_M" --timeout 30


* `--video_dir`: (Optional) Directory containing the video files and their pickled summaries. Defaults to `datasets/XD_Violence_1-1004`.

* `--ollama_url`: (Optional) URL for the Ollama chat API endpoint. Defaults to `http://localhost:11434/api/chat`.

* `--ollama_model`: (Optional) Name of the Ollama model to use. Defaults to `gemma3:4b-it-q4_K_M`.

* `--timeout`: (Optional) Timeout for Ollama API requests in seconds. Defaults to 30.

This will load the chunked summaries, run the anomaly detection, print the LLM's judgments, save `*_anomaly_scores.pkl`, and generate an `*_anomaly_scores.png` plot in the video directory.

**Note:** Ensure you have videos in the specified `video_dir` and that `process_video` has been run at least once to generate the summary `.pkl` files before running `anomaly_detection`.





